{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding for the Digital Humanities Workshop\n",
    "\n",
    "- [Website link](https://dh-coding-docs.netlify.app/)\n",
    "- [Literature Review CSV](DH-lit-review.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: pandas in /usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: langdetect in /usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: iso_language_codes in /usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "# install the packages\n",
    "!pip3 install nltk\n",
    "!pip3 install pandas\n",
    "!pip3 install langdetect\n",
    "!pip3 install iso_language_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/aidanpower/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aidanpower/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/aidanpower/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# import everything and define dataframe variables\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.text import Text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from langdetect import detect\n",
    "from iso_language_codes import language_name\n",
    "\n",
    "from string import punctuation\n",
    "punctuation = list(punctuation)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([\"n't\", \"'s\", 'would', '—', \"“\", \"”\", '\"'])\n",
    "\n",
    "path = \"DH-lit-review.csv\"\n",
    "full_df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution #1: list of abstracts\n",
    "\n",
    "My original solution working with the abstract list directly from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full list of abstracts: 50\n",
      "Filtered without NaN: 40\n",
      "Filtered for English only: 29\n",
      "\n",
      "Number of sentences: 179\n",
      "Number of words: 6215\n"
     ]
    }
   ],
   "source": [
    "# get the abstracts from the dataframe, remove NaN and non-english language abstracts\n",
    "full_list = list(full_df[\"Description\"])\n",
    "valid_list = list(filter(lambda x: str(x) != 'nan', full_list))\n",
    "en_list = list(filter(lambda x: detect(x) == 'en', valid_list))\n",
    "print(\"Full list of abstracts: \" + str(len(full_list)) + \"\\nFiltered without NaN: \" + str(len(valid_list)) + \"\\nFiltered for English only: \" + str(len(en_list)))\n",
    "\n",
    "abstract_list = list(map(lambda x: x.lower(), en_list))\n",
    "\n",
    "# merge the abstracts, tokenize the sentences and words\n",
    "abstract_string = \" \".join(abstract_list)\n",
    "abstract_sent = sent_tokenize(abstract_string)\n",
    "abstract_words = word_tokenize(abstract_string)\n",
    "print(\"\\nNumber of sentences: \" + str(len(abstract_sent)) + \"\\nNumber of words: \" + str(len(abstract_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution #2: working within the dataframe\n",
    "\n",
    "My updated solution working wihtin the dataframe to fill out more informaion such as publication year and language of each abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full list of abstracts: 50\n",
      "Filtered without NaN: 40\n",
      "Filtered for English only: 29\n",
      "\n",
      "Number of sentences: 179\n",
      "Number of words: 6215\n"
     ]
    }
   ],
   "source": [
    "# adds the year to the \"Creation Date\" column by extracting it from the \"Is part of\" column\n",
    "def add_year(df):\n",
    "    new_df = df.copy()\n",
    "    for index, row in df.iterrows():\n",
    "        string = row[\"Is part of\"]\n",
    "        match = re.match(r'.*([2][0][0-9]{2})', string)\n",
    "        if match is not None:\n",
    "            year = int(match.group(0)[-4:])\n",
    "            new_df.loc[index, \"Year\"] = year\n",
    "    # was having an issue with the year value appearing as a float rather than an int\n",
    "    new_df['Year'] = new_df['Year'].astype(int)\n",
    "    return(new_df)\n",
    "\n",
    "# do the same process but filter via the dataframe and add the language to the Language Note column\n",
    "valid_df = full_df[(full_df[\"Description\"].notna())].copy().reset_index(drop=True)\n",
    "\n",
    "# rename the columns and add the language ISO code and name\n",
    "valid_df = valid_df.rename(columns={\"Language Note\": \"Language ISO\", \"Exhibitions Note\": \"Language Full\", \"Creation Date\": \"Year\", \"Creator\": \"Authors\", \"Description\": \"Abstract\"})\n",
    "valid_df[\"Language ISO\"] = valid_df[\"Abstract\"].apply(lambda x: detect(x))\n",
    "valid_df[\"Language Full\"] = valid_df[\"Language ISO\"].apply(lambda x: language_name(x))\n",
    "valid_df = add_year(valid_df)\n",
    "\n",
    "# filter for English only and drop empty columns\n",
    "en_df = valid_df[(valid_df[\"Language ISO\"] == 'en')].copy().reset_index(drop=True)\n",
    "dataframe = en_df #.dropna(axis=1, how='all').reset_index(drop=True)\n",
    "print(\"Full list of abstracts: \" + str(len(full_df.index)) + \"\\nFiltered without NaN: \" + str(len(valid_df.index)) + \"\\nFiltered for English only: \" + str(len(dataframe.index)))\n",
    "\n",
    "# merge the abstracts, tokenize the sentences and words\n",
    "abstract_list = list(map(lambda x: x.lower(), dataframe[\"Abstract\"]))\n",
    "\n",
    "abstract_string = \"\\n\".join(abstract_list)\n",
    "abstract_sent = sent_tokenize(abstract_string)\n",
    "abstract_words = word_tokenize(abstract_string)\n",
    "print(\"\\nNumber of sentences: \" + str(len(abstract_sent)) + \"\\nNumber of words: \" + str(len(abstract_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langauge Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Italian</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Danish</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>German</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Language  Count\n",
       "0     English     29\n",
       "1     Italian      4\n",
       "2      Danish      3\n",
       "3     Spanish      2\n",
       "4  Portuguese      1\n",
       "5      German      1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of entires in each language\n",
    "lang_count = Counter(valid_df[\"Language Full\"])\n",
    "lang_df = pd.DataFrame.from_dict(lang_count, orient=\"index\").reset_index().rename(columns={\"index\": \"Language\", 0: \"Count\"})\n",
    "lang_df.sort_values(by=\"Count\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the tokenized word list and get the regular and stemmed frequency distributions \n",
    "filtered_abstract_words = []\n",
    "stemmed_abstract_words = []\n",
    "for word in abstract_words:\n",
    "    if word not in stop_words and word not in punctuation:\n",
    "        stem = ps.stem(word)\n",
    "        stemmed_abstract_words.append(stem)\n",
    "        filtered_abstract_words.append(word)\n",
    "\n",
    "abstract_fdist = FreqDist(filtered_abstract_words)\n",
    "stemmed_fdist = FreqDist(stemmed_abstract_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract frequency distribution\n",
      "95: humanities\n",
      "90: digital\n",
      "56: research\n",
      "42: dh\n",
      "26: analysis\n",
      "25: field\n",
      "24: topic\n",
      "24: text\n",
      "23: study\n",
      "19: new\n",
      "19: reading\n",
      "18: work\n",
      "18: data\n",
      "18: texts\n",
      "16: also\n",
      "15: studies\n",
      "15: dhp-lclw\n",
      "14: paper\n",
      "13: topics\n",
      "13: exploration\n"
     ]
    }
   ],
   "source": [
    "print(\"Abstract frequency distribution\")\n",
    "for value in abstract_fdist.most_common(20):\n",
    "    print(str(value[1]) + \": \" + str(value[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemmed frequency distribution\n",
      "101: human\n",
      "92: digit\n",
      "59: research\n",
      "42: dh\n",
      "42: text\n",
      "39: studi\n",
      "37: topic\n",
      "32: explor\n",
      "32: use\n",
      "26: field\n",
      "26: analysi\n",
      "21: read\n",
      "21: visual\n",
      "19: work\n",
      "19: new\n",
      "19: differ\n",
      "18: practic\n",
      "18: develop\n",
      "18: data\n",
      "16: also\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStemmed frequency distribution\")\n",
    "for value in stemmed_fdist.most_common(20):\n",
    "    print(str(value[1]) + \": \" + str(value[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 10 of 15 matches:\n",
      " for mr. lo chia-lun 's writings ( dhp-lclw ) with and without the ata to assi\n",
      "ness of text exploration using the dhp-lclw with and without the ata varied si\n",
      "c of the text being explored . the dhp-lclw with the ata was found to be more \n",
      "oring historical texts , while the dhp-lclw without the ata was more suitable \n",
      " exploring educational texts . the dhp-lclw with the dhp-lclw was found to be \n",
      "onal texts . the dhp-lclw with the dhp-lclw was found to be significantly more\n",
      "s of perceived usefulness than the dhp-lclw without the ata , indicating that \n",
      "for mr. lo chia-lun ’ s writings ( dhp-lclw ) . htat can assist humanities sch\n",
      " conducting text exploration using dhp-lclw with htat or dhp-lclw with single-\n",
      "ration using dhp-lclw with htat or dhp-lclw with single-layer topic analysis t\n"
     ]
    }
   ],
   "source": [
    "# concordance\n",
    "text_list = Text(abstract_words)\n",
    "text_list.concordance(\"dhp-lclw\", lines=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching the dataframe\n",
    "\n",
    "I wrote a quick little function that searches the dataframe and returns relevant articles where it appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Year</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Communicating Uncertainty in Digital Humanitie...</td>\n",
       "      <td>Panagiotidou, Georgia ;  Lamqaddam, Houda ;  P...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Due to their historical nature, humanistic dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>An associative text analyzer to facilitate eff...</td>\n",
       "      <td>Chen, Chih-Ming ;  Chen, Xian-Xu</td>\n",
       "      <td>2024</td>\n",
       "      <td>PurposeThis study aims to develop an associati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Curating China's Cultural Revolution (1966–197...</td>\n",
       "      <td>Ma, Rongqian</td>\n",
       "      <td>2022</td>\n",
       "      <td>CR/10 is a digital oral history platform that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>A hierarchical topic analysis tool to facilita...</td>\n",
       "      <td>Chen, Chih-Ming ;  Ho, Szu-Yu ;  Chang, Chung</td>\n",
       "      <td>2023</td>\n",
       "      <td>PurposeThis study aims to develop a hierarchic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "12  Communicating Uncertainty in Digital Humanitie...   \n",
       "22  An associative text analyzer to facilitate eff...   \n",
       "27  Curating China's Cultural Revolution (1966–197...   \n",
       "28  A hierarchical topic analysis tool to facilita...   \n",
       "\n",
       "                                              Authors  Year  \\\n",
       "12  Panagiotidou, Georgia ;  Lamqaddam, Houda ;  P...  2023   \n",
       "22                   Chen, Chih-Ming ;  Chen, Xian-Xu  2024   \n",
       "27                                       Ma, Rongqian  2022   \n",
       "28      Chen, Chih-Ming ;  Ho, Szu-Yu ;  Chang, Chung  2023   \n",
       "\n",
       "                                             Abstract  \n",
       "12  Due to their historical nature, humanistic dat...  \n",
       "22  PurposeThis study aims to develop an associati...  \n",
       "27  CR/10 is a digital oral history platform that ...  \n",
       "28  PurposeThis study aims to develop a hierarchic...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# searches the df for word in abstract and returns relevant titles, authors, publication years and full abstracts\n",
    "def search(df, word):\n",
    "    subset = df.filter([\"Title\", \"Authors\", \"Year\", \"Abstract\"])\n",
    "    index_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        abstract = row[\"Abstract\"]\n",
    "        if str(abstract) != 'nan' and word in abstract.lower():\n",
    "            index_list += [index]\n",
    "\n",
    "    return subset.iloc[index_list]\n",
    "\n",
    "search(dataframe, \"historical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the dataframe of english artilces with publication year added to file\n",
    "path = \"DH-lit-review-output.csv\"\n",
    "dataframe.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
